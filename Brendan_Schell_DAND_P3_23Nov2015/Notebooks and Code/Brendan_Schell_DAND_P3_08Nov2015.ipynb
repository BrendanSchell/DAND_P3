{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAND P3\n",
    "## Brendan Schell\n",
    "\n",
    "Map Area: Toronto, ON, Canada\n",
    "\n",
    "<a href=\"\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problems Encountered in the Map\n",
    "\n",
    "After parsing through a small sample size of the Toronto .osm file, I noticed the following problems with the street map data:\n",
    "\n",
    "    * Incorrect Postal Codes\n",
    "    * Use of addr:State instead of addr:Province\n",
    "    * Use of an 'address' tag instead of 'addr:' sub-components\n",
    "    \n",
    "### Problem 1: Incorrect Postal Codes\n",
    "\n",
    "After parsing through the first 1000 \"addr:postcode\" keys in way or node objects and comparing them to a regular expression for Canadian postal codes (found [here](http://geekswithblogs.net/MainaD/archive/2007/12/03/117321.aspx)), I noticed that there were some invalid Canadian Postal Codes.\n",
    "\n",
    ">p = audit_postal_codes(filename) \n",
    "\n",
    ">for k in p.keys(): \n",
    ">>print k + \":\" + str(len(p[k]))\n",
    "\n",
    "Valid: 762\n",
    "Invalid: 5\n",
    "\n",
    "Of the 5 invalid postal codes observed for the first 100,000 nodes and ways parsed: \n",
    "\n",
    "L7J 1B9  : A valid postal code, but contains a trailing space\n",
    "\n",
    "M36 0H7 : Contains 4 numbers - an invalid postal code\n",
    "\n",
    "l7a 3r9 : lower case letters\n",
    "\n",
    "l6p 2r1 : lower case letters\n",
    "\n",
    "M5T 1R9, M1P 2L7 : multiple postal codes\n",
    "\n",
    "Using the small amount of invalid postal codes above as a sample size, I decided to change all lower case letters in postal codes to be upper case. This was done using the following code:\n",
    "\n",
    "> if v == 'addr:postcode':\n",
    ">> node[v] = i.get('v').upper()\n",
    "\n",
    "\n",
    "### Problem 2: Use of addr:state for provinces\n",
    "\n",
    "A second problem observed when going through the tag data was the use of the addr:state tag (seen below).\n",
    "\n",
    "* addr:street\n",
    "* addr:housenumber\n",
    "* addr:city\n",
    "* addr:state\n",
    "* addr:country\n",
    "* addr:postcode\n",
    "* addr:housename\n",
    "* addr:province\n",
    "* address\n",
    "* addr:building\n",
    "* addr:unitAfter \n",
    "\n",
    "After parsing through the first 10 uses of this tag, all of them were populated with the value \"ON\" which is the correct value for the province. Since Canada has provinces and not states and there are are also addr:province tags, these should be consolidated. This was corrected by making an exception for state tags on import so that they would become province tags instead.\n",
    "\n",
    "\n",
    "### Problem 3: Use of 'address' tag instead of 'addr:' fields\n",
    "\n",
    "After collecting the first few instances of the address tag, these were the results:\n",
    "\n",
    "* 1386 Queen Street West\n",
    "* 1303 Queen Street West\n",
    "* 309 Augusta Avenue\n",
    "* 56 Spadina Road\n",
    "* 45 Main Street East\n",
    "* 2840 Dundas Street West\n",
    "* 1996 Itabashi Way\n",
    "* 975 Cosburn Avenue\n",
    "* 131 Chisholm Avenue\n",
    "* 3733 Ransomville Road, Ransomville, NY 14131\n",
    "\n",
    "Of the above, the majority are valid addresses, but are not separated according to the addr: specifications. To correct this, any addresses that begin with a number and end with a valid street name are be converted into addr: housenumber and addr:street tags.\n",
    "\n",
    "The following code was used to solve the three observed issues on import to JSON:\n",
    "\n",
    "> if v == 'addr:postcode':\n",
    ">> node[v.replace('addr:','')] = i.get('v').upper()\n",
    ">elif v == 'address':\n",
    ">>if v[0:v.find(\" \")].isnumeric:\n",
    ">>>house_num = v[0:v.find(\" \")]\n",
    ">>>temp = fix_address(v[v.find(\" \")+1:])\n",
    ">>>if temp != -1:\n",
    ">>>>node['housenumber'] = house_num\n",
    ">>>>node['streetname'] = temp\n",
    ">elif v == 'addr:state':\n",
    ">>node['province'] = i.get('v')\n",
    ">else:\n",
    ">>address[v.replace('addr:','')] = i.get('v')\n",
    "\n",
    "\n",
    "## 2. Data Overview\n",
    "\n",
    "This section contains basic statistics about the dataset and the MongoDB queries used to gather them.\n",
    "\n",
    "In this section, coll is the collection invoked by:\n",
    "\n",
    ">db = client['street_data']\n",
    "\n",
    ">coll = db.P3\n",
    "\n",
    "#### File Sizes\n",
    "OSM: 1.2 GB\n",
    "\n",
    "JSON: 1.3 GB\n",
    "#### Number of Documents\n",
    "Result: 6081859\n",
    "\n",
    "Query:\n",
    ">coll.find().count()\n",
    "\n",
    "#### Number of Ways\n",
    "Result: 646403\n",
    "\n",
    "Query:\n",
    ">coll.find({\"type\":\"way\"}).count()\n",
    "\n",
    "#### Number of Nodes\n",
    "Result: 5435456\n",
    "\n",
    "Query:\n",
    ">coll.find({\"type\":\"node\"}).count()\n",
    "\n",
    "#### Number of Unique Users\n",
    "Result:\n",
    "1663\n",
    "Query:\n",
    ">len(coll.distinct(\"created.user\"))\n",
    "\n",
    "#### Top five 'city' field values\n",
    "Result: \n",
    ">{u'count': 5653353, u'_id': None}\n",
    "\n",
    ">{u'count': 112419, u'_id': u'City of Toronto'}\n",
    "\n",
    ">{u'count': 40241, u'_id': u'City of Hamilton'}\n",
    "\n",
    ">{u'count': 36869, u'_id': u'Mississauga'}\n",
    "\n",
    ">{u'count': 25997, u'_id': u'City of Brampton'}\n",
    "\n",
    "Query:\n",
    ">cursor = coll.aggregate(\n",
    ">>    [\n",
    ">>>        {'$group': {'_id': '$address.city', 'count' : {'$sum' : 1}}},\n",
    "      {'$sort' : {'count' : -1}},\n",
    "      {'$limit' : 5}\n",
    ">>    ]\n",
    ">for doc in cursor:\n",
    ">>    print doc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 3. Additional Ideas\n",
    "\n",
    "### Improvement Suggestion: Iterative input of postal codes\n",
    "\n",
    "By looking at the postal code data in Section 1, I observed that many addresses were missing postal codes.\n",
    "\n",
    "The number of addresses included within the City of Toronto, determined using Query 1 below is 112,419. The number of postal codes present within these, given by query 2 is 50. It is clear from these queries that very few addresses in the open street data contain the correct postal code information.\n",
    "\n",
    "One potential activity that could be done to improve the data would be to retrieve postal code data for these addresses using Google Maps or another data source and update the open street map data accordingly. This could be done using a python script similar to what was used for this project.\n",
    "\n",
    ">1) coll.find({'address.city': 'City of Toronto'}).count()\n",
    "\n",
    ">2) coll.find({'address.city': 'City of Toronto', 'postcode' : {'$exists':True}}).count()\n",
    "\n",
    "### Additional data exploration using MongoDB queries\n",
    "\n",
    "#### Determining use of Gluten Free designation in Open Street Map Data\n",
    "\n",
    ">cursor = coll.aggregate(\n",
    "\n",
    ">>    [\n",
    "\n",
    ">>        {'$group': {'_id': '$diet:gluten_free', 'count' : {'$sum' : 1}}},\n",
    "        {'$sort' : {'count' : -1}},\n",
    "        {'$limit' : 5}\n",
    ">>    ]) \n",
    "\n",
    ">for i in cursor:\n",
    "\n",
    ">>    print i\n",
    "\n",
    ">{u'count': 6081857, u'_id': None}\n",
    "\n",
    ">{u'count': 2, u'_id': u'yes'}\n",
    "\n",
    "Result: Only used for 2 entries.\n",
    "\n",
    "#### Top five 'street' field values\n",
    "\n",
    ">cursor = coll.aggregate(\n",
    ">>    [\n",
    ">>>        {'$group': {'_id': '$address.street', 'count' : {'$sum' : 1}}},\n",
    "      {'$sort' : {'count' : -1}},\n",
    "      {'$limit' : 5}\n",
    ">>    ]\n",
    ">for doc in cursor:\n",
    ">>    print doc\n",
    "\n",
    "\n",
    "\n",
    ">{u'count': 5650184, u'_id': None}\n",
    "\n",
    ">{u'count': 2208, u'_id': u'Yonge Street'}\n",
    "\n",
    ">{u'count': 1020, u'_id': u'Bathurst Street'}\n",
    "\n",
    ">{u'count': 999, u'_id': u'Dundas Street West'}\n",
    "\n",
    ">{u'count': 919, u'_id': u'Bloor Street West'}\n",
    "\n",
    "#### Number of entries with wheelchair descriptions:\n",
    "\n",
    ">cnt = -1 #account for 'None' value\n",
    "\n",
    ">cursor = coll.aggregate(\n",
    "\n",
    ">>    [\n",
    "\n",
    ">>>        {'$group': {'_id': '$wheelchair:description', 'count' : {'$sum' : 1}}},\n",
    "\n",
    ">>>        {'$sort' : {'count' : -1}},\n",
    "\n",
    ">>>        {'$limit' : 5}\n",
    "\n",
    ">>    ]) \n",
    "\n",
    ">for i in cursor:\n",
    "\n",
    ">>    cnt += 1\n",
    "\n",
    ">print cnt\n",
    "\n",
    "Result: 4\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In conclusion, the data was successfully cleaned, migrated into MongoDB, and queried against to determine some interesting information about the city of Toronto. Based on the small number of tags containing non-address and non-user information, it does not seem as though the Open Street Map data for Toronto is very populated with accessory information. Much could be done using available information from APIs to improve the level of detail in this Open Street Map data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
